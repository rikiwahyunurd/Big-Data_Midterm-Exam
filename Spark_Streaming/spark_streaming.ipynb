{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb2366d9-307d-4999-935d-22bdd8b3518b",
   "metadata": {},
   "source": [
    "# Create Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b30d2011",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://LAPTOP-2L95E1II:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Streaming Process Files</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1311253c940>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Streaming Process Files\") \\\n",
    "    .config(\"spark.streaming.stopGracefullyOnShutdown\", True) \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbaeebf4-59e3-4f67-b67c-65d798013c1d",
   "metadata": {},
   "source": [
    "# Create the DataFrameStreamReader dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8601753f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To allow automatic schemaInference while reading\n",
    "spark.conf.set(\"spark.sql.streaming.schemaInference\", True)\n",
    "\n",
    "# Create the streaming_df to read from input directory\n",
    "streaming_df = spark.readStream\\\n",
    "    .format(\"json\") \\\n",
    "    .option(\"cleanSource\", \"archive\") \\\n",
    "    .option(\"sourceArchiveDir\", \"data/archive/\") \\\n",
    "    .option(\"maxFilesPerTrigger\", 1) \\\n",
    "    .option(\"multiline\",\"true\")\\\n",
    "    .option(\"inferSchema\", \"true\")\\\n",
    "    .load(\"data/input-05111940000188/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea7fd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To the schema of the data, place a sample json file and change readStream to read \n",
    "streaming_df.printSchema()\n",
    "streaming_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de7fd95-2949-4efc-8655-e8d04a9d8dc0",
   "metadata": {},
   "source": [
    "# Flatten the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1b62975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets explode the data as devices contains list/array of device reading\n",
    "from pyspark.sql.functions import explode, col\n",
    "\n",
    "exploded_df = streaming_df \\\n",
    "    .select(\"link\", \"headline\", \"category\", \"short_description\", \"authors\", \"date\")\n",
    "    # .withColumn(\"devices\", explode(\"data.devices\")) \\\n",
    "    # .drop(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0fd3b520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the exploded df\n",
    "flattened_df = exploded_df \\\n",
    "    .selectExpr(\"link\", \"headline\", \"category\", \"short_description\", \"authors\", \"date\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad009e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the output to console sink to check the output\n",
    "writing_df = flattened_df.writeStream \\\n",
    "    .format(\"json\") \\\n",
    "    .option(\"path\", \"data/output-05111940000188\") \\\n",
    "    .option(\"checkpointLocation\",\"checkpoint_dir\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .start()\n",
    "    \n",
    "# Start the streaming application to run until the following happens\n",
    "# 1. Exception in the running program\n",
    "# 2. Manual Interruption\n",
    "writing_df.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cfa1560c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------+---------+----------+----------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|authors                                |category |date      |headline                                                                    |link                                                                                                          |short_description                                                                                                                                                                                       |\n",
      "+---------------------------------------+---------+----------+----------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|Eric Tucker and Mary Clare Jalonick, AP|U.S. NEWS|2022-09-21|Virginia Thomas Agrees To Interview With Jan. 6 Panel                       |https://www.huffpost.com/entry/virginia-thomas-agrees-to-interview-with-jan-6-panel_n_632ba0f2e4b09d8701bbe16d|Conservative activist Virginia Thomas, the wife of Supreme Court Justice Clarence Thomas, has agreed to participate in a voluntary interview with the House panel investigating the Jan. 6 insurrection.|\n",
      "|Carla K. Johnson, AP                   |U.S. NEWS|2022-09-23|Over 4 Million Americans Roll Up Sleeves For Omicron-Targeted COVID Boosters|https://www.huffpost.com/entry/covid-boosters-uptake-us_n_632d719ee4b087fae6feaac9                            |Health experts said it is too early to predict whether demand would match up with the 171 million doses of the new boosters the U.S. ordered for the fall.                                              |\n",
      "|                                       |U.S. NEWS|2022-09-04|Emergency Declared As Flash Flooding Hits Northwest Georgia                 |https://www.huffpost.com/entry/georgia-flash-flooding_n_63151514e4b0eac9f4cdc7d3                              |Thunderstorms and heavy rain pounded parts of northwest Georgia on Sunday, sparking flash flooding and dangerously high waters in some areas.                                                           |\n",
      "+---------------------------------------+---------+----------+----------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check the data at the output location\n",
    "\n",
    "out_df = spark.read.json(\"data/output-05111940000188/\")\n",
    "out_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ae0914-f6d6-4cde-b135-bae26cf072d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
